{"test_size": 0.2, "random_state": 42, "epochs": 10, "batch_size": 32, "verbose": 1, "dense_layer_units": 64, "dropout_rate": 0.2, "num_classes": 8, "patience": 3, "min_delta": 0.001, "model_save_path": "F:\\Projects\\Major_DevSpell\\project\\project\\llama-3.1-70b-versatile\\training_model\\best_model.h5"}