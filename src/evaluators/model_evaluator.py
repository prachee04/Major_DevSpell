import numpy as np
import matplotlib.pyplot as plt

import seaborn as sns
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score, 
    confusion_matrix
)

class ModelEvaluator:
    def evaluate(self, projects):
        """
        Evaluate multiple ML projects generated by different LLMs
        """
        results = {}
        
        for llm, project in projects.items():
            # Perform model training and evaluation
            model = project.train_model()
            # predictions = model.predict(test_data)
            
            # results[llm] = {
            #     'accuracy': accuracy_score(test_labels, predictions),
            #     'precision': precision_score(test_labels, predictions),
            #     'recall': recall_score(test_labels, predictions),
            #     'f1_score': f1_score(test_labels, predictions),
            #     'confusion_matrix': confusion_matrix(test_labels, predictions)
            # }
        
        self.visualize_comparison(results)
        return results
    
    def visualize_comparison(self, results):
        """
        Create visualizations comparing model performances
        """
        metrics = ['accuracy', 'precision', 'recall', 'f1_score']
        
        plt.figure(figsize=(12, 6))
        for metric in metrics:
            values = [results[llm][metric] for llm in results]
            plt.bar(list(results.keys()), values, label=metric)
        
        plt.title("Model Performance Comparison")
        plt.ylabel("Score")
        plt.legend()
        plt.tight_layout()
        plt.show()