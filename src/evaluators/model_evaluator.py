import concurrent.futures
import logging
from .nlp_model_evaluator import NLPModelEvaluator
from .recommendation_model_evaluator import RecommendationModelEvaluator
from .classification_evaluator import ClassificationModelEvaluator
from .time_series_evaluator import TimeSeriesModelEvaluator
from .computer_vision_evaluator import ComputerVisionModelEvaluator

class ModelEvaluator:
    """Centralized class for evaluating ML models generated by different LLMs."""

    def __init__(self):
        # Map project types to specific evaluators
        self.evaluators = {
            'nlp': NLPModelEvaluator,
            'recommendation': RecommendationModelEvaluator,
            'classification': ClassificationModelEvaluator,
            'time_series': TimeSeriesModelEvaluator,
            'computer_vision': ComputerVisionModelEvaluator,
        }

    def evaluate_projects(self, projects_dict):
        """
        Evaluate projects in parallel using their corresponding evaluators.

        Args:
            projects_dict (dict): Dictionary of projects keyed by LLM names.

        Returns:
            dict: Evaluation results for each LLM.
        """
        results = {}

        # Use ThreadPoolExecutor to evaluate models in parallel
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_to_llm = {
                executor.submit(self._evaluate_single_project, llm, project): llm
                for llm, project in projects_dict.items()
            }

            # Collect evaluation results as they complete
            for future in concurrent.futures.as_completed(future_to_llm):
                llm = future_to_llm[future]
                try:
                    results[llm] = future.result()
                except Exception as e:
                    logging.error(f"Error evaluating project for {llm}: {e}")
                    results[llm] = {"error": str(e)}

        return results

    def _evaluate_single_project(self, llm, project):
        """
        Evaluate a single project's model.

        Args:
            llm (str): Name of the LLM.
            project (dict): Details of the project generated by the LLM.

        Returns:
            dict: Evaluation metrics for the project.
        """
        project_type = project.get('project_type', 'recommendation')  # Default to recommendation
        evaluator_class = self.evaluators.get(project_type)

        if not evaluator_class:
            raise ValueError(f"No evaluator found for project type: {project_type}")

        # Instantiate the evaluator and evaluate the project
        evaluator = evaluator_class(project)
        return evaluator.evaluate_model()

    def generate_comparison_visualizations(self, results):
        """
        Generate comparison visualizations across LLMs.

        Args:
            results (dict): Evaluation metrics for each LLM.

        Returns:
            dict: Paths to the generated visualizations.
        """
        import pandas as pd
        import matplotlib.pyplot as plt
        import seaborn as sns
        import os

        # Convert results to a DataFrame
        metrics_df = pd.DataFrame(results).T

        # Create visualization directory if not exists
        viz_dir = "results/visualizations"
        os.makedirs(viz_dir, exist_ok=True)

        # Plot comparison of metrics
        metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score']
        metrics_df = metrics_df[metrics_to_plot].dropna()

        plt.figure(figsize=(10, 6))
        metrics_df.plot(kind='bar', figsize=(12, 6), title="Model Performance Comparison")
        plt.xlabel("LLM")
        plt.ylabel("Scores")
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"{viz_dir}/performance_comparison.png")
        plt.close()

        return {'metrics_summary': metrics_df.to_dict(), 'visualization_path': f"{viz_dir}/performance_comparison.png"}
